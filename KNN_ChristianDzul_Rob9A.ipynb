{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# **K-Nearest Neighbors(KNN)**"
      ],
      "metadata": {
        "id": "f3gGtXedvCtV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# We must know...\n",
        "\n",
        "KNN is a flexible technique that may be applied to both regression and classification tasks. Since it is a non-parametric model, it does not make the same pre-data assumptions as a linear regression model, such as that the pre-data must be linear. It is a supervised learning technique; therefore, you must provide it labeled data to work with. In the training data, measurements are made between x and y.\n",
        "\n",
        "The objective is to identify a function h:X -> Y that enables a positive prediction of the same output y from an unknown observation. We use the Euclidean metric to discover this link. The same method is used to resolve classifications and regression; you simply specify the number of neighbors you're looking for, and those neighbors serve as a factor for classifications. Two arrays are given, and the difference between each element is squared before taking the square root.\n",
        "\n",
        "Repeating the calculations while iterating K along the odd numbers within a specified range is the most effective technique to determine the ideal K value. A large number of neighbors usually causes chaos. When you've determined the ideal value, you can utilize it for more training and testing.\n",
        "\n",
        "To sum up, the implementation of KNN involves the following steps:\n",
        "\n",
        "* Calculate the distance (Euclidean) between a test data point and every training data point. This is to see who is closer and who is far by how much.\n",
        "* Sort the distances and pick K nearest distances (first K entries) from it. Those will be K closest neighbors to your given test data point.\n",
        "* Get the labels of the selected K neighbors. The most common label (label with a majority vote) will be the predicted label for our test data point.\n",
        "* Repeat everything above for all the test data points in your test set.\n"
      ],
      "metadata": {
        "id": "NwEZyyANAECa"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Pseudocode\n",
        "\n",
        "1. Define the KNN class:\n",
        "* Create a class named \"KNN\" to encapsulate the K-Nearest Neighbors algorithm.\n",
        "\n",
        "2. Constructor - `neighbors(self, k=5)`:\n",
        "* Create a constructor to set the number of neighbors (k), with a default value of 5.\n",
        "\n",
        "3. Fit the Model - `fit(self, X, y)`:\n",
        "* Create a method to fit the KNN model with training data.\n",
        "* Store the training data (X) in the class attribute `self.X_train`.\n",
        "* Store the corresponding labels (y) in the class attribute `self.y_train`.\n",
        "\n",
        "4. Calculate Euclidean Distance - `Euclidean_distance(self, X1, X2)`:\n",
        "* Create a method to calculate the Euclidean distance between two data points (X1 and X2).\n",
        "* Initialize a variable `distance` to 0.\n",
        "* Loop through the features of the data points and compute the squared difference.\n",
        "* Return the square root of the computed distance.\n",
        "\n",
        "5. Make Predictions - `prediction(self, X_test)`:\n",
        "* Create a method to make predictions for the test data.\n",
        "* Initialize an empty list `results` to store predictions.\n",
        "* Loop through each test data point in `X_test`.\n",
        "* For each test data point, calculate the distances to all training data points.\n",
        "* Store the distances and corresponding training data indices in the `distances` list.\n",
        "* Sort the distances in ascending order.\n",
        "* Select the first `k` distances (k nearest neighbors).\n",
        "* Retrieve the labels of the k nearest neighbors and store them in the `neighbors` list.\n",
        "* Predict the class label as the majority class among the k nearest neighbors.\n",
        "* Append the prediction to the `results` list.\n",
        "* Return the list of predictions.\n",
        "\n",
        "6. Calculate Accuracy - `accuracy(self, X_test, y_test)`:\n",
        "* Create a method to calculate the accuracy of the model.\n",
        "* Make predictions for the test data using the `prediction` method.\n",
        "* Count the number of correct predictions by comparing predictions with actual labels.\n",
        "* Calculate the accuracy as the ratio of correct predictions to the total number of test data points.\n",
        "\n",
        "7. Dataset Preprocessing:\n",
        "* Import the dataset from a CSV file (e.g., \"diabetes.csv\").\n",
        "* Select a subset of the dataset (e.g., 100 data points) for training and testing.\n",
        "\n",
        "8. Feature Selection and Label Extraction:\n",
        "* Extract the features (X) and labels (y) from the dataset.\n",
        "\n",
        "9. Split Data into Training and Test Sets:\n",
        "* Split the data into training and test sets, e.g., using a function like `divide_training_&_test_sets`.\n",
        "* Set a random seed for reproducibility and specify the test set size (e.g., 20% of the data).\n",
        "\n",
        "10. Initialize the KNN Model:\n",
        "* Create an instance of the KNN class with a specified number of neighbors (e.g., k=5).\n",
        "\n",
        "11. Fit the Model:\n",
        "* Fit the KNN model with the training data using the `fit` method.\n",
        "\n",
        "12. Make Predictions:\n",
        "* Use the `prediction` method to make predictions for the test data.\n",
        "* Display each prediction.\n",
        "\n",
        "13. Calculate and Display Accuracy:\n",
        "* Use the `accuracy` method to calculate the accuracy of the model on the test data.\n",
        "* Display the accuracy of the model.\n"
      ],
      "metadata": {
        "id": "bmW1RDsu77D_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# CODE EXPLANATION"
      ],
      "metadata": {
        "id": "m4Wzvpt5ArdK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In the code I use the numpy, pandas, and sklearn_model_selection libraries.  \n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "CtwpAuXBB6j2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "import pandas as pd"
      ],
      "metadata": {
        "id": "ycPO0zD_vT7O"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Explanation\n",
        "\n",
        "* We import numpy to start and then create the class for our KNearestNeighbors algorithm. The model is built around the number of neighbors we are looking to find. Let k equal the assumed number of classifications, in this case be set as k = 5.\n",
        "\n",
        "* Secondly, with the fit function I ensure that the length of X and y are equal, otherwise we will have trouble down the road. X_train, y_train are what we will call this moving forward, and it will add these values to the class.\n",
        "\n",
        "* Thirdly, with the distance function, I calculate the Euclidean metrics for two arrays. Its needed to take in two values. These are converted to np.arrays. Then, instantiate the distance at a 0 value, which is the default distance for two numbers of the same value. We then need to find the difference between the two values and square them. This gives us the absolute value and makes the differences act larger. Finally, we return the squared values for the distances.\n",
        "\n",
        "* We want the X_test inputs to be passed to the predcit function. To start, we create a blank list for our sorted_outputs. The length of X_test is then iterated over in a for loop. You want to make two additional empty lists in this for loop: one for distances and one for neighbors. These will store the results of our distance calculations and execute our forecasts. The length of the X_train data that should already be fitted to the model must be passed via another for loop that must be nested inside of this one. Utilizing that specific instance of the X_train data and the aggregate distance of the X_test data, each iteration should calculate a distance.\n",
        "\n",
        "  This calculation is added to the distances list. It is  important to sort this list; sklearn approach this from a different angle but this was the most efficient way that I found. Once it is sorted, you want to slice the list down to the most relevant datapoints, which will be 0:k (k being the total number of neighbors you are solving for). Once is sorted, you then run another for loop for each instance in the sliced distances where it appends the y_train value of the instance to a list. This essentially gives  a list of possible neighbors. Once I take the max of that list, and append that value to your sorted outputs, I can return outputs and obtain a prediction on a given set of data.\n",
        "\n",
        "\n",
        "* with the last function called 'score', we obtain the accuracy for the predictions. It should accept X_test and y_test arguments. It should create a list of predictions taken by running the X_test and running it through the predict method. The accuracy is returned by taking the predictions with the y_test and summing the values. Then, divide this by the length of the y_test to give us a percentage. Scoring only works if we have both the test values and their relative labels.\n"
      ],
      "metadata": {
        "id": "GJBuvqSjB3Wm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class KNN:\n",
        "    def __init__(self, k=5):\n",
        "        self.k = k\n",
        "\n",
        "    def fit(self, X, y):\n",
        "        self.X_train = X\n",
        "        self.y_train = y\n",
        "\n",
        "    def distance(self, X1, X2):\n",
        "        X1, X2 = np.array(X1), np.array(X2)\n",
        "        distance = 0\n",
        "        for i in range(len(X1) - 1):\n",
        "            distance += (X1[i] - X2[i]) ** 2\n",
        "        return np.sqrt(distance)\n",
        "\n",
        "    def predict(self, X_test):\n",
        "        sorted_output = []\n",
        "        for i in range(len(X_test)):\n",
        "            distances = []\n",
        "            neighbors = []\n",
        "            for j in range(len(self.X_train)):\n",
        "                dist = self.distance(self.X_train[j], X_test[i])\n",
        "                distances.append([dist, j])\n",
        "            distances.sort()\n",
        "            distances = distances[0:self.k]\n",
        "            for distances, j in distances:\n",
        "                neighbors.append(self.y_train[j])\n",
        "            ans = max(neighbors)\n",
        "            sorted_output.append(ans)\n",
        "\n",
        "        return sorted_output\n",
        "\n",
        "    def score(self, X_test, y_test):\n",
        "        predictions = self.predict(X_test)\n",
        "        return (predictions == y_test).sum() / len(y_test)"
      ],
      "metadata": {
        "id": "Bb_vJI2-vnj4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "* I load the 'diabetes' dataset and I run it through train_test_split. I used a test value of 20%.\n",
        "\n",
        "* I first cast the class to a variable and set the number of neighbors. For this situatuin I used 5, and odd number.\n",
        "\n",
        "* I fit the model to our training data.\n",
        "\n",
        "* I test to see that the predictions processes are working. This test yields an array with diabetes or not tags represented as 1 or 0 for successfully guessing the proper classification for the dataset I chose.\n",
        "\n",
        "* I run the scoring method to see how well I did.\n",
        "\n",
        "* I check that the classifications predictions were correct or not.\n"
      ],
      "metadata": {
        "id": "EvtnFn1WFMub"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Importing dataset, I used the diabetes dataset\n",
        "dataset = pd.read_csv(\"diabetes.csv\", sep=\",\")\n",
        "dataset = dataset.head(120)\n",
        "X = dataset[['Pregnancies', 'Glucose', 'BloodPressure', 'SkinThickness', 'Insulin', 'BMI','DiabetesPedigreeFunction','Age']].values\n",
        "y = dataset['Outcome'].values\n",
        "X_train, X_test, y_train, y_test = train_test_split( X, y, random_state=40, test_size=0.2)\n",
        "\n",
        "# Initialize model\n",
        "testing_neighbors = KNN(k=5)\n",
        "\n",
        "# We fit the model to the training data\n",
        "testing_neighbors.fit(X_train, y_train)\n",
        "\n",
        "# We score the prediction accuracy.\n",
        "score = testing_neighbors.score(X_test, y_test)\n",
        "print('The accuracy of the model is :', score)\n",
        "\n",
        "# Run predictions using the test sample data.\n",
        "prediction = testing_neighbors.predict(X_test)\n",
        "print('\\nPredictions:\\n', prediction)\n",
        "\n",
        "\n",
        "prediction == y_test\n",
        "\n",
        "#Clasification results\n",
        "print(\"\\nClasification Results:\")\n",
        "for cls in np.unique(y_test):\n",
        "    correct_count = np.sum((y_test == cls) & (prediction == cls))\n",
        "    incorrect_count = np.sum((y_test == cls) & (prediction != cls))\n",
        "    print(f\"Clase {cls}: {correct_count} clasificados correctamente, {incorrect_count} incorrectos\")"
      ],
      "metadata": {
        "id": "KDKZjX4JvsB_",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "040d3a19-d93c-4d13-e634-9f95a5aea540"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The accuracy of the model is : 0.7916666666666666\n",
            "\n",
            "Predictions:\n",
            " [1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
            "\n",
            "Clasification Results:\n",
            "Clase 0: 5 clasificados correctamente, 5 incorrectos\n",
            "Clase 1: 14 clasificados correctamente, 0 incorrectos\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# LOSS + OPTIMIZATION FUNCTION\n",
        "\n",
        "Loss functions and optimization functions are not used in the K-Nearest Neighbors (KNN) algorithm because:\n",
        "\n",
        "1. KNN is instance-based and doesn't learn model parameters.\n",
        "2. KNN doesn't have a training phase; it memorizes training data.\n",
        "3. Predictions are directly based on nearest neighbors, not model parameters.\n",
        "4. Hyperparameters, like 'k,' are typically chosen without optimization.\n",
        "\n",
        "Loss and optimization functions are used in supervised learning models to adjust model parameters during training, which doesn't apply to KNN's simple, non-parametric approach."
      ],
      "metadata": {
        "id": "1TcQF9YgBL1w"
      }
    }
  ]
}